Yesterday, we discussed the importance of the activation function. Now, letâ€™s discuss some of the important activation functions:

Sigmoid function: Squeeze input value into the range from 0 to 1, generally used when we are interested in the probability of something.
Hyperbolic Tangent: Squeeze input value into the range from -1 to +1; it converges faster than sigmoid.
Rectified Linear Unit (ReLU): It outputs 0 for negative inputs and the value itself for positive inputs, which generally helps prevent gradient vanishing and fast training.

<img width="1089" height="313" alt="image" src="https://github.com/user-attachments/assets/4c144a00-8738-4427-9f3a-2d9023f9f6a0" />
